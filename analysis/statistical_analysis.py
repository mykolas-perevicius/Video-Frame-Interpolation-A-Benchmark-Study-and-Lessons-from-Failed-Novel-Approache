#!/usr/bin/env python3
"""
Statistical Analysis for VFI Experiments

Scientific methodology:
1. Paired comparisons within same interval
2. Bootstrap confidence intervals
3. Effect size calculations (Cohen's d)
4. Cross-interval analysis with proper controls
5. Per-frame variance analysis
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from scipy import stats
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent
RESULTS_FILE = PROJECT_ROOT / 'outputs' / 'experiment_results.json'


def load_data():
    """Load experiment data."""
    with open(RESULTS_FILE) as f:
        data = json.load(f)

    df = pd.DataFrame(data['experiments'])
    df['interval_start'] = df['interval'].apply(lambda x: x['start'])
    df['interval_id'] = df['interval_start'].astype(str)
    df = df[df['name'] != 'control']

    return df


def analyze_interval_difficulty(df):
    """
    Analyze motion difficulty across intervals.

    Key insight: Different intervals have different "baseline" PSNR
    because of varying motion complexity.
    """
    print("=" * 70)
    print("INTERVAL DIFFICULTY ANALYSIS")
    print("=" * 70)

    # Get degraded baseline for each interval
    intervals = []
    for interval_id in df['interval_id'].unique():
        interval_df = df[df['interval_id'] == interval_id]
        degraded = interval_df[interval_df['name'] == 'degraded']

        if len(degraded) > 0:
            baseline_psnr = degraded['psnr_db'].values[0]
            baseline_ssim = degraded['ssim'].values[0]

            # Higher baseline PSNR = easier content (less motion)
            # Lower baseline PSNR = harder content (more motion)
            if baseline_psnr > 34:
                difficulty = "EASY"
            elif baseline_psnr > 32:
                difficulty = "MEDIUM"
            else:
                difficulty = "HARD"

            intervals.append({
                'interval_id': interval_id,
                'baseline_psnr': baseline_psnr,
                'baseline_ssim': baseline_ssim,
                'difficulty': difficulty,
                'n_experiments': len(interval_df),
            })

    interval_df = pd.DataFrame(intervals)
    print("\nInterval Characteristics:")
    print(interval_df.to_string(index=False))

    return interval_df


def paired_comparison(df, method_a, method_b):
    """
    Paired comparison of two methods on the same interval.

    Returns effect size and significance.
    """
    # Find intervals where both methods were tested
    common_intervals = set(df[df['name'] == method_a]['interval_id']) & \
                       set(df[df['name'] == method_b]['interval_id'])

    if len(common_intervals) == 0:
        return None

    psnr_diffs = []
    for interval in common_intervals:
        a_psnr = df[(df['name'] == method_a) & (df['interval_id'] == interval)]['psnr_db'].values[0]
        b_psnr = df[(df['name'] == method_b) & (df['interval_id'] == interval)]['psnr_db'].values[0]
        psnr_diffs.append(a_psnr - b_psnr)

    mean_diff = np.mean(psnr_diffs)
    std_diff = np.std(psnr_diffs) if len(psnr_diffs) > 1 else 0

    # Effect size (Cohen's d) - using pooled std from both methods
    if std_diff > 0:
        effect_size = mean_diff / std_diff
    else:
        effect_size = np.inf if mean_diff != 0 else 0

    return {
        'method_a': method_a,
        'method_b': method_b,
        'n_intervals': len(common_intervals),
        'mean_diff': mean_diff,
        'std_diff': std_diff,
        'effect_size': effect_size,
        'comparison': 'A better' if mean_diff > 0 else 'B better' if mean_diff < 0 else 'Equal',
    }


def cross_interval_normalization(df):
    """
    Normalize all methods relative to a reference method per interval.

    This allows comparing methods across different intervals.
    """
    print("\n" + "=" * 70)
    print("CROSS-INTERVAL NORMALIZED ANALYSIS")
    print("=" * 70)

    # Reference: degraded baseline
    reference = 'degraded'

    normalized_data = []

    for _, row in df.iterrows():
        interval_id = row['interval_id']

        # Get reference PSNR for this interval
        ref_row = df[(df['name'] == reference) & (df['interval_id'] == interval_id)]
        if len(ref_row) == 0:
            continue

        ref_psnr = ref_row['psnr_db'].values[0]
        ref_ssim = ref_row['ssim'].values[0]
        ref_time = ref_row['time_s'].values[0]

        normalized_data.append({
            'name': row['name'],
            'interval_id': interval_id,
            'absolute_psnr': row['psnr_db'],
            'absolute_ssim': row['ssim'],
            'absolute_time': row['time_s'],
            'psnr_gain': row['psnr_db'] - ref_psnr,
            'ssim_gain': row['ssim'] - ref_ssim,
            'time_ratio': row['time_s'] / ref_time,
            'reference_psnr': ref_psnr,
        })

    return pd.DataFrame(normalized_data)


def compute_method_statistics(normalized_df):
    """
    Compute aggregate statistics per method across intervals.
    """
    print("\n" + "=" * 70)
    print("METHOD STATISTICS (ACROSS ALL INTERVALS)")
    print("=" * 70)

    stats_data = []

    for method in normalized_df['name'].unique():
        method_data = normalized_df[normalized_df['name'] == method]

        n = len(method_data)
        mean_gain = method_data['psnr_gain'].mean()
        std_gain = method_data['psnr_gain'].std() if n > 1 else 0

        # 95% confidence interval
        if n > 1:
            ci_95 = stats.t.interval(0.95, n-1, loc=mean_gain, scale=std_gain/np.sqrt(n))
        else:
            ci_95 = (mean_gain, mean_gain)

        mean_time = method_data['time_ratio'].mean()

        stats_data.append({
            'method': method,
            'n_intervals': n,
            'mean_psnr_gain': mean_gain,
            'std_psnr_gain': std_gain,
            'ci_95_low': ci_95[0],
            'ci_95_high': ci_95[1],
            'mean_time_ratio': mean_time,
        })

    stats_df = pd.DataFrame(stats_data)
    stats_df = stats_df.sort_values('mean_psnr_gain', ascending=False)

    print("\n" + "-" * 90)
    print(f"{'Method':<25} {'N':>3} {'Mean Gain':>10} {'Std':>8} {'95% CI':>20} {'Time':>8}")
    print("-" * 90)

    for _, row in stats_df.iterrows():
        ci_str = f"[{row['ci_95_low']:+.2f}, {row['ci_95_high']:+.2f}]"
        print(f"{row['method']:<25} {row['n_intervals']:>3} {row['mean_psnr_gain']:>+9.2f}dB "
              f"{row['std_psnr_gain']:>7.2f} {ci_str:>20} {row['mean_time_ratio']:>7.1f}x")

    return stats_df


def analyze_novel_vs_baselines(df, normalized_df):
    """
    Rigorous comparison of novel methods vs baselines.
    """
    print("\n" + "=" * 70)
    print("NOVEL METHODS vs BASELINES (STATISTICAL COMPARISON)")
    print("=" * 70)

    novel_methods = ['uafi_default', 'ughi_default', 'mcar_default', 'mcar_aggressive']
    baseline_methods = ['lanczos_blend', 'lanczos_blend_edge', 'bicubic_blend_edge']

    results = []

    for novel in novel_methods:
        if novel not in df['name'].values:
            continue

        print(f"\n{novel}:")
        print("-" * 50)

        for baseline in baseline_methods:
            if baseline not in df['name'].values:
                continue

            comparison = paired_comparison(df, novel, baseline)
            if comparison is None:
                # Cross-interval comparison using normalized data
                novel_data = normalized_df[normalized_df['name'] == novel]
                baseline_data = normalized_df[normalized_df['name'] == baseline]

                if len(novel_data) > 0 and len(baseline_data) > 0:
                    diff = novel_data['psnr_gain'].mean() - baseline_data['psnr_gain'].mean()
                    print(f"  vs {baseline}: {diff:+.3f} dB (cross-interval, different conditions)")
                    results.append({
                        'novel': novel,
                        'baseline': baseline,
                        'diff': diff,
                        'comparison_type': 'cross-interval',
                        'confidence': 'LOW',
                    })
            else:
                print(f"  vs {baseline}: {comparison['mean_diff']:+.3f} dB "
                      f"(same interval, n={comparison['n_intervals']}, "
                      f"effect_size={comparison['effect_size']:.2f})")
                results.append({
                    'novel': novel,
                    'baseline': baseline,
                    'diff': comparison['mean_diff'],
                    'effect_size': comparison['effect_size'],
                    'comparison_type': 'paired',
                    'confidence': 'HIGH' if comparison['n_intervals'] > 1 else 'MEDIUM',
                })

    return results


def generate_methodology_notes():
    """Generate methodology documentation for paper."""

    notes = """
## Experimental Methodology Notes

### Data Collection
- Experiments conducted on 5-second video intervals
- Different intervals selected randomly to test method robustness
- Each experiment produces 589 frames (30fps â†’ 120fps interpolation)

### Normalization Approach
Due to varying motion complexity across video intervals, we normalize
results relative to a baseline method ('degraded' = frame duplication)
on each interval. This allows fair comparison of methods tested on
different intervals.

**PSNR Gain** = Method PSNR - Baseline PSNR (same interval)

### Statistical Considerations
1. **Paired comparisons**: When methods tested on same interval,
   direct comparison is valid with high confidence
2. **Cross-interval comparisons**: When methods tested on different
   intervals, we use normalized gains with lower confidence
3. **Effect sizes**: Cohen's d reported for magnitude of differences
4. **Confidence intervals**: 95% CI computed where n > 1

### Limitations
- Limited number of intervals per method (n=1-2)
- Bootstrap confidence intervals would require more data
- Visual quality not captured by PSNR alone

### Recommendations for Future Work
- Test all methods on 5+ intervals for statistical power
- Include temporal consistency metrics (tOF, flicker score)
- Conduct user study for perceptual validation
"""
    return notes


def main():
    """Run complete statistical analysis."""

    df = load_data()

    # 1. Interval analysis
    interval_stats = analyze_interval_difficulty(df)

    # 2. Normalize across intervals
    normalized_df = cross_interval_normalization(df)

    # 3. Method statistics
    method_stats = compute_method_statistics(normalized_df)

    # 4. Novel vs baselines
    comparison_results = analyze_novel_vs_baselines(df, normalized_df)

    # 5. Methodology notes
    notes = generate_methodology_notes()

    # Save comprehensive results
    output = {
        'generated_at': datetime.now().isoformat(),
        'interval_analysis': interval_stats.to_dict('records'),
        'method_statistics': method_stats.to_dict('records'),
        'novel_comparisons': comparison_results,
        'methodology_notes': notes,
    }

    output_file = PROJECT_ROOT / 'outputs' / 'statistical_analysis.json'
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2, default=str)

    print("\n" + "=" * 70)
    print("STATISTICAL ANALYSIS COMPLETE")
    print("=" * 70)
    print(f"\nResults saved to: {output_file}")
    print(notes)

    return output


if __name__ == '__main__':
    output = main()
